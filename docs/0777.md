# 理解 R 中的线性回归

> 原文:[https://www . edu reka . co/blog/understanding-linear-regression-in-r/](https://www.edureka.co/blog/understanding-linear-regression-in-r/)

线性回归通过评估自变量在类似情况下的表现来收集因变量的输出。线性回归是基于直线方程 y = mx+c，这里 y 是因变量，x 是自变量。

[//www.youtube.com/embed/Q-A2eEHNnWk](//www.youtube.com/embed/Q-A2eEHNnWk)

线性回归是基于普通最小二乘回归。了解以下类型的变量也很重要:

**因变量**–因变量是回归模型中要预测或解释的变量。假设该变量与自变量在函数上相关。

**自变量**–自变量是回归方程中与因变量相关的变量。自变量在回归模型中用于估计因变量的值。

例如，我们采用两种变量，如降雨量和小麦产量。小麦产量变量是因变量，降雨量是自变量。也可以有一个以上的自变量。当只有一个独立变量时，它被称为简单线性回归。如果有一个以上的变量，这就是所谓的多元线性回归。

让我们举一个例子，一位教授试图向他的学生展示期中考试的重要性。他认为期中成绩越高，期末成绩也越高。从他班上随机抽取了 15 名学生，数据包括他们的期中成绩和期末成绩。

假设期中成绩高会导致期末成绩高。

使用期末成绩和期中成绩变量创建散点图。如果用户需要在下一节课中根据期中成绩预测最终成绩，他可以根据以前的数据设计一个线性回归模型。

我们总是把因变量放在 Y 轴上，自变量放在 X 轴上。

这里有一个正相关。回归是相关的进一步步骤。如果两个项目是相关的，我们希望找到它们的数学方程，那么我们使用回归。

曲线(负斜率)表示两个变量不一定相互关联，可以具有非线性关系。无关系图显示变量不相关时的情况。开发线性回归模型的假设:

1.  变量必须具有线性关系
2.  如果数据量很大，我们用 R，应该有办法拒绝/接受模型进行预测。

![linear regression](../Images/0363a7acef748d02c4ec97ab47cd369f.png)线性关系的类型有:

**正线性关系**–表示两个变量之间的正相关关系。

**关系** **不是** **线性——**有一种关系我们可以把点放在一个数学方程里。

**负线性关系—**表示两个变量之间的负相关关系。

**无关系—**表示两个变量之间没有关系。

**直线方程**

我们从方程 Y = a + b X 开始

Y =因变量

A = Y 轴截距

B =直线的斜率

X =独立变量

Y & X 线性无关，线性相关，相关度高。A & B 是常数项，等式中没有指数符号。

在多元回归模型中，有多个 x 以 x1、x2、x3 等形式出现。

截距–当我们通过 Y 轴画一条线，回归线和 Y 轴之间的距离称为截距。

直线的斜率–它是回归线和 X 轴之间的角度，是直线的斜率。

让我们以简单的线性回归为例，我们需要检查杂货店的年销售额与其平方英尺大小的线性相关性。7 家商店的样本数据通过商店数量、平方英尺和年销售额等元素获得。

散点图表示与异常值正相关。

有问题要问我们吗？？在评论区提到它们，我们会给你回复。

**相关帖子:**

[商业分析与 R 培训](https://www.edureka.co/r-for-analytics)

[商业分析学导论](https://www.edureka.co/blog/videos/introduction-business-analytics-with-r/ "Introduction to Business Analytics with R")