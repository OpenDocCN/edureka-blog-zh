# AWS 中的大数据—大数据的智能解决方案

> 原文:[https://www.edureka.co/blog/big-data-in-aws](https://www.edureka.co/blog/big-data-in-aws)

大数据的概念并不新鲜，它无处不在。大数据的影响无处不在，从商业到科学，从政府到艺术等等。没有比 AWS 更好的伴侣来处理和分析大数据。从[大数据课程](https://www.edureka.co/big-data-hadoop-training-certification)中了解大数据专家使用的所有工具和系统。

在本文中，我将展示 AWS 如何应对大数据的挑战，我将介绍的要点如下:

*   [什么是大数据？](#what)
*   [为什么要在 AWS 中使用大数据？](#why)
*   [AWS 如何解决大数据挑战？](#how)
*   [演示](#demo)

## **什么是大数据？**

![big data characteristics](../Images/e6448f83c4306da46a6e3926166a3fa5.png)

您可以将大数据视为高容量、高速度和/或高多样性的信息资产，这些资产需要经济高效、创新的信息处理形式来增强洞察力、决策制定和流程自动化。

大数据由定义大数据特征的 5 个重要 V 组成。在转向 AWS 之前，让我们讨论一下这些问题。

### **![Five-Vs-of-Big-Data-What-is-Big-Data-Edureka](../Images/eea8259b09426f8fcffbcd1f07e13e88.png)T2】**

### **什么是 AWS？**

![AWS Logo - Aws Migration - Edureka](../Images/fa640691425b94c2059608853bfe5533.png) [AWS](https://www.edureka.co/blog/what-is-aws/) 由许多不同的云计算产品和服务组成。利润丰厚的亚马逊部门提供服务器、存储、网络、远程计算、电子邮件、移动开发以及安全服务。此外。AWS 由两个主要产品组成:亚马逊的虚拟机服务 EC2 和亚马逊的存储系统 S3。它是如此之大，出现在计算世界中，以至于现在它的规模至少是最接近的竞争对手的 10 倍，并且托管着像网飞和 Instagram 这样的热门网站。

。![AWS market - Big Data in AWS - Edureka](../Images/102ca15914a1d4ad202e929b3cc95446.png)

AWS 分为全球 12 个区域，每个区域都有多个服务器所在的可用性区域。这些服务区域被分割，以允许用户对其服务设置地理限制，同时通过分散保存数据的物理位置来提供安全性。你可以通过孟买 的 [数据工程培训更好的了解。](https://www.edureka.co/microsoft-azure-data-engineering-certification-course-mumbai)

**查看我们在顶级城市的 AWS 认证培训**

| 印度 | 美国 | 其他国家 |
| [在海德拉巴的 AWS 培训](https://www.edureka.co/aws-certification-training-hyderabad) | [亚特兰大 AWS 培训](https://www.edureka.co/aws-certification-training-atlanta) | [AWS 伦敦培训](https://www.edureka.co/aws-certification-training-london) |
| [班加罗尔的 AWS 培训](https://www.edureka.co/aws-certification-training-bangalore) | [波士顿 AWS 培训](https://www.edureka.co/aws-certification-training-boston) | [阿德莱德的 AWS 培训](https://www.edureka.co/aws-certification-training-adelaide) |
| [钦奈的 AWS 培训](https://www.edureka.co/aws-certification-training-chennai) | [纽约市的 AWS 培训](https://www.edureka.co/aws-certification-training-new-york-city) | [新加坡 AWS 培训](https://www.edureka.co/aws-certification-training-singapore) |

## **为什么要在 AWS 中使用大数据？**

来自许多不同领域的科学家、开发人员和其他技术爱好者正在利用 AWS 来执行大数据分析，并应对数字信息不断增长的严峻挑战。AWS 为您提供云计算服务组合，通过显著降低成本、扩展以满足需求和提高创新速度来帮助管理大数据。

![](../Images/5c1a089802874d94ec4e9bcd7efdb4a8.png)

亚马逊网络服务提供了一个完全整合的云计算服务组合。此外，它还能帮助您构建、保护和部署大数据应用程序。此外，有了 AWS，您不需要购买硬件，也不需要维护和扩展基础架构。因此，你可以将你的资源集中在发现新的见解上。由于新功能不断增加，您将始终能够利用最新技术，而无需做出长期投资承诺。

## **AWS 如何解决大数据挑战？**

### **面向大数据的 AWS 解决方案**

AWS 有许多用于所有开发和部署目的的解决方案。此外，在数据科学和大数据领域，AWS 已经在大数据处理的不同方面取得了最新进展。在开始使用工具之前，让我们了解一下 AWS 可以为大数据的不同方面提供解决方案。

1.  **数据摄取** ![Ingestion - Big Data in AWS - Edureka](../Images/87bd683a2e8702c71c8c9328267b8432.png)收集原始数据(交易、日志、移动设备等)是许多组织在处理大数据时面临的第一个挑战。一个好的大数据平台使这一步变得更容易，允许开发人员以任何速度从实时到批量接收各种数据，从结构化到非结构化。

2.  **数据存储** ![Data Storage - Big Data in AWS - Edureka](../Images/d722a466ac00fe372b5d941353856927.png)任何大数据平台都需要一个安全、可扩展、持久的存储库，在处理任务之前甚至之后存储数据。根据您的具体要求，您可能还需要临时存储传输中的数据。

3.  **数据处理** ![Data processing - Big Data in AWS - Edureka](../Images/ae2b77a1722820929ae3672c532357b6.png)这是数据从原始状态转换为可消费格式的步骤，通常通过排序、聚合、连接甚至执行更高级的功能和算法来实现。生成的数据集经过存储以供进一步处理，或通过商业智能和数据可视化工具供消费使用。

4.  #### **可视化**

    ![visualization - Big Data in AWS - Edureka](../Images/fa73f0d80e466848be20e6adec343f4f.png)

    大数据就是从您的数据资产中获得高价值、可操作的见解。理想情况下，利益相关方可以通过自助式商业智能和灵活的数据可视化工具获得数据，从而快速轻松地探索数据集。

你甚至可以通过 [数据工程师课程](https://www.edureka.co/microsoft-azure-data-engineering-certification-course) 了解大数据的细节。

### **面向大数据的 AWS 工具**

在前面的章节中，我们介绍了 AWS 可以提供解决方案的大数据领域。此外，AWS 拥有多种工具和服务，可为客户提供大数据功能。从芝加哥的[大数据培训中了解更多关于工具和应用的信息。](https://www.edureka.co/big-data-and-hadoop-chicago)

让我们看看 AWS 为处理大数据的不同阶段提供的各种解决方案

#### **摄入**

1.  **Kinesis**

    亚马逊 Kinesis Firehose 是一项完全托管的服务，用于直接向亚马逊 S3 提供实时流数据。Kinesis Firehose 可自动扩展以匹配流数据的容量和吞吐量，无需持续管理。您可以配置 Kinesis Firehose 来转换流数据，然后再将其存储在亚马逊 S3。

2.  **雪球![snowball - Big Data in AWS - Edureka](../Images/be50044f24a7b6a6663929d9956b8d91.png)** 您可以使用 [AWS 雪球](https://www.edureka.co/blog/aws-snowball-and-snowmobile-tutorial/)安全高效地将批量数据从内部存储平台和 Hadoop 集群迁移到 S3 存储桶。在 AWS 管理控制台中创建一个作业后，您会自动获得一个雪球设备。雪球到达后，将其连接到您的本地网络，在您的本地数据源上安装雪球客户端，然后使用雪球客户端选择文件目录并将其传输到雪球设备。

#### **存储**

1.  **亚马逊 S3T2】**

![S3-Big Data in AWS - Edureka](../Images/c5d0ab6dc8ca5f0094102a1e6e2b3829.png) [亚马逊 S3](https://www.edureka.co/blog/s3-aws-amazon-simple-storage-service/) 是一个安全、高度可扩展、持久的对象存储，数据访问延迟为毫秒级。S3 可以存储来自任何地方的任何类型的数据-网站和移动应用程序、企业应用程序以及来自物联网传感器或设备的数据。它还可以存储和检索任意数量的数据，具有无与伦比的可用性，并且从头开始构建，可提供 99.999999999% (11 个 9)的耐用性。

**2。AWS 胶水**

Glue 是一个完全托管的服务，它提供了一个数据目录，使得数据湖中的数据可以被发现。此外，它还能够执行提取、转换和加载(ETL ),为分析准备数据。此外，内置的数据目录就像所有数据资产的持久元数据存储，使所有数据都可以在单个视图中搜索和查询。

#### **处理**

1.  **EMR** ![EMR - Big Data in AWS - Edureka](../Images/28ea0320d20d96be5d8c97f035c6d416.png)对于使用 Spark 和 Hadoop 的大数据处理，[亚马逊 EMR](https://aws.amazon.com/emr/) 提供托管服务，使处理大量数据变得简单、快速且经济高效。此外，EMR 支持 19 个不同的开源项目，包括 [Hadoop](https://aws.amazon.com/emr/details/hadoop/) 、 [Spark](https://aws.amazon.com/emr/details/spark/) 和它还附带了用于数据工程、数据科学开发和协作的托管 EMR 笔记本。

2.  **红移** ![redshift - Big Data in AWS - Edureka](../Images/71771fbfd64381973577b8ca14a65ca2.png)对于数据仓库，[亚马逊](https://aws.amazon.com/redshift/)红移提供了针对数 Pb 结构化数据运行复杂分析查询的能力。此外，它还包括[红移谱](https://aws.amazon.com/redshift/spectrum/)，可以直接对 S3 数十亿字节的结构化或非结构化数据运行 SQL 查询，而无需不必要的数据移动。

#### **可视化**

1.  **亚马逊 QuickSight** ![quicksight - Big Data in AWS - Edureka](../Images/0c8c6c3c27c1eebefdb8950088d58722.png)

    对于仪表盘和可视化，Amazon Quicksight 为您提供了快速、云驱动的业务分析服务。它使构建令人惊叹的可视化和丰富的仪表板变得容易。此外，您可以从任何浏览器或移动设备访问它们。

详细的，你甚至可以通过 [AWS 云迁移](https://www.edureka.co/migrating-to-aws)来查看迁移到 AWS 的细节。

## **演示-分析澳大利亚濒危动植物物种的数据。**

在这个演示中，我们将使用来自澳大利亚各州和地区的濒危动植物物种的样本数据。在这里，我们将创建一个 EMR 集群，并将其配置为运行多步 Apache Hive 作业。EMR 集群将安装 Apache Hive。该集群将使用 EMRFS 作为文件系统，以便将其数据输入和输出位置映射到 S3 存储桶。集群也将使用相同的 S3 存储桶来存储日志文件。

![](../Images/2b1c73b931f9ddbd7c65cd478bdae5a3.png)

我们现在将在集群中创建许多 EMR 步骤来处理一组样本数据。这里的每一步都将运行一个 Hive 脚本，最终的输出将被保存到 S3 存储桶中。这些步骤将生成 MapReduce 日志，这是因为 Hive 命令在运行时被转换为 MapReduce 作业。每个步骤的日志文件都是从它生成的容器中聚合而来的。

### **样本数据**

此使用案例的样本数据集可从  [澳大利亚政府的开放数据网站](https://data.gov.au/)公开获取。这个数据集是关于澳大利亚不同州和地区的濒危动植物物种的。可以在  [这里](https://data.gov.au/dataset/threatened-species-state-lists/resource/c478a397-e1e9-4292-950c-e0f9ed9430c8)查看和下载该数据集的字段描述和 CSV 文件。

### **处理步骤**

这里的第一个 EMR 作业步骤包括创建一个 Hive 表，作为 S3 中底层源文件的模式。在第二个作业步骤中，我们现在将针对数据运行一个成功的查询。类似地，我们将运行第三和第四个查询。

我们将在一小时内多次重复这四个步骤，模拟多步骤批处理作业的连续运行。然而，在现实生活中，每个批处理运行之间的时间差通常会更大。连续运行之间的小时间间隔旨在加速我们的测试。

### **S3 桶和文件夹**

![create S3 bucket - Big Data in AWS - Edureka](../Images/5852c11420d4b87a5ba3e468f968d09b.png)

在创建我们的 EMR 集群之前，我们必须创建一个 S3 存储桶来存放它的文件。在我们的示例中，我们将该存储桶命名为“arvind1-bucket”。在 S3 的 AWS 控制台中，该存储桶下的文件夹如下所示:

![create folder in S3 - Big Data in AWS - Edureka](../Images/d1a482f3b35d0ab32010078e6a7df55d.png)

*   输入文件夹保存样本数据

*   脚本文件夹包含 EMR 作业步骤的配置单元脚本文件

*   输出文件夹显然会保存配置单元程序的输出

*   EMR 群集使用日志文件夹来保存其日志文件。

### **EMR 作业步骤的配置单元脚本**

1。该作业步骤运行一个配置单元脚本 来创建一个外部配置单元表。此表描述了基础 CSV 数据文件的表格模式。脚本如下:

```
 CREATE EXTERNAL TABLE `threatened_species`( `scientific name` string, `common name` string, `current scientific name` string, `threatened status` string, `act` string, `nsw` string, `nt` string, `qld` string, `sa` string, `tas` string, `vic` string, `wa` string, `aci` string, `cki` string, `ci` string, `csi` string, `jbt` string, `nfi` string, `hmi` string, `aat` string, `cma` string, `listed sprat taxonid` bigint, `current sprat taxonid` bigint, `kingdom` string, `class` string, `profile` string, `date extracted` string, `nsl name` string, `family` string, `genus` string, `species` string, `infraspecific rank` string, `infraspecies` string, `species author` string, `infraspecies author` string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3://arvind1-bucket/script/'
```

2.该作业步骤运行一个查询来计算新南威尔士州(NSW)的五大濒危物种。蜂巢查询文件名为  `endangeredSpeciesNSW.q` ，如下图所示:

```
SELECT species, COUNT(nsw)AS number_of_endangered_species FROM threatened_species WHERE (nsw = 'Yes'  OR nsw = 'Endangered') AND "threatened status" = 'Endangered' GROUP BY species HAVING COUNT(nsw) &gt; 1 ORDER BY number_of_endangered_species DESC LIMIT 5
```

3.该作业步骤运行一个查询来计算澳大利亚每个植物家族的濒危植物物种总数。蜂巢查询文件名为 `endangeredPlantSpecies.q` ，如下图

```
 SELECT family, COUNT(species) AS number_of_endangered_species FROM threatened_species2 WHERE kingdom = 'Plantae' AND "threatened status" = 'Endangered' GROUP BY family
```

4.这一步列出了澳大利亚昆士兰州灭绝动物物种的学名。脚本文件名为  `extinctAnimalsQLD.q` ，如下所示:

```
 SELECT "common name", "scientific name" FROM threatened_species WHERE kingdom = 'Animalia' AND (qld = 'Yes' OR qld = 'Extinct') AND "threatened status" = 'Extinct'
```

### **日志聚合**

在这里，我们还上传了一个名为 logAggregation.json 的 JSON 文件，它位于 S3 存储桶的 scripts 文件夹中。我们使用这个文件来聚集纱线日志文件。当集群启动时，在 yarn-site.xml 配置文件中配置日志聚合。logAggregation.json 文件的内容如下:

[ { "Classification": "yarn-site "，" Properties ":{ " yarn . log-aggregation-enable ":" true "，" yarn . log-aggregation . retain-seconds ":"-1 "，" yarn . node manager . remote-app-log-dir ":" S3://arvind 1-bucket/logs " }]]

在您创建了 S3 存储桶并将数据和脚本文件复制到各自的文件夹后，现在是时候设置 EMR 集群了。以下快照描述了我们使用大部分默认设置创建集群的过程。

### **EMR 集群设置**

![create EMR cluster - Big Data in AWS - Edureka](../Images/0000431222778903bf3fae7f766fa936.png)

在第一幅图中，为了在 AWS 控制台中配置集群，我们保留了 EMR 推荐的所有应用程序，包括 Hive。我们不需要使用 AWS Glue 来存储配置单元元数据，也不需要添加任何作业步骤。但是，我们需要为 Hive 添加一个软件设置。在这里，您必须仔细观察我们是如何在这个字段中指定日志聚合 JSON 文件的路径的。

![](../Images/beb3273adfc21bcbf7cb40611f1b5d73.png)

在下一步中，我们保留了所有默认设置。为了我们的测试，集群将有一个主节点和两个核心节点。这里的每个节点都是一个 m3.xlarge 实例，有 10 GB 的根卷。在下一步中，我们将集群命名为 arvind1-cluster，并为其日志文件指定定制的 s3 位置。

最后，为了访问集群的主节点，我们指定了一个 EC2 密钥对。EMR、EC2 实例配置文件和自动缩放选项的默认 IAM 角色没有变化。此外，默认情况下，主节点和核心节点使用可用的安全组。通常，这是 EMR 集群的默认设置。一切就绪后，集群将处于“等待”状态，如下所示:

![](../Images/e0171a17b9862abd783ca113e174d658.png)T2】

### **提交配置单元作业步骤**

之后，我们需要允许 SSH 访问。

1.  在[](https://console.aws.amazon.com/elasticmapreduce/)打开亚马逊 EMR 控制台。
2.  选择  **集群**。
3.  选择  **名称** 的集群。
4.  在  **安全和** 权限下选择  **安全组为** 主链接。
5.  从列表中选择**ElasticMapReduce-master**。
6.  选择**编辑**。
7.  找到具有以下设置的规则并选择  **x** 图标将其删除:

    *   **类型** SSH
    *   **端口** 22
    *   **源**自定义 0.0.0.0/0

8.  滚动到规则列表的底部，选择  **添加规则**。
9.  对于  **类型**，选择  **SSH** 。这样自动进入  **TCP** 为  **协议** 和  **22** 为  **端口范围**。
10.  对于来源，选择  **我的 IP** 。这将自动添加您的客户端电脑的 IP 地址作为源地址。或者，您可以添加一系列  **自定义** 可信客户端 IP 地址，并选择  **添加规则** 为其他客户端创建附加规则。在许多网络环境中，您动态分配 IP 地址，因此您可能需要定期编辑安全组规则来更新受信任客户端的 IP 地址。
11.  选择  **保存**。
12.  或者，从列表中选择**ElasticMapReduce-slave**，并重复上述步骤，以允许 SSH 客户端从可信客户端访问核心和任务节点。

由于 EMR 集群已经启动并运行，我们已经添加了四个作业步骤。这些是 EMR 将一个接一个运行的步骤。下图显示了从 AWS EMR 控制台执行的步骤:

![](../Images/2569c5fa2441e833093d2e10ebcc080a.png)T2】

一旦我们添加了这四个步骤，我们就可以检查这些步骤的完成状态。即使这些步骤的执行存在一些问题，在这种情况下也可以使用这些步骤的日志文件来解决。

这就是我在这篇关于 AWS 中的大数据的文章中的观点。我希望你已经理解了我在这里解释的一切。

*如果你在 AWS 中发现了这种大数据的相关内容，你可以查看 Edureka 在 [AWS 在线培训](https://www.edureka.co/aws-certification-training)上的直播和讲师指导课程，该课程由行业从业者共同创建。*

*有问题吗？请在如何在 AWS 中部署 Java Web 应用程序的评论部分提到它，我们会回复您。*T3】