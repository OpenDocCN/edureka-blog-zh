# 2023 年 Apache PIG 上的顶级 Hadoop 面试问题

> 原文：<https://www.edureka.co/blog/interview-questions/hadoop-interview-questions-pig/>

## **阿帕奇猪面试问题**

寻找雇主经常问的阿帕奇猪面试问题？ 这里是 Hadoop 面试问题系列的第五篇博客，内容涵盖阿帕奇猪面试问题。这份问题清单是经过大量研究，并在已经在业界活跃数年的*认证大数据 Hadoop* 专家的严格指导下精心编制的。希望大家一定没有错过我们 **[Hadoop 面试问题](https://www.edureka.co/blog/interview-questions/top-50-hadoop-interview-questions-2016/)** **[系列](https://www.edureka.co/blog/interview-questions/top-50-hadoop-interview-questions-2016/)** 的早前博客。

在经历了 Pig 面试问题后，你将深入了解 Hadoop 面试中雇主经常问的问题。

如果您以前参加过 Pig 访谈，我们鼓励您在评论标签中添加您的问题。我们很乐意回答这些问题，并向求职者传播这个信息。

## **关于阿帕奇猪要记住的要点:**

Apache Pig 是一个平台，用于分析大型数据集，将它们表示为数据流。它旨在提供对 MapReduce 的抽象，降低使用 Java 编程编写 MapReduce 任务的复杂性。我们可以使用 Apache Pig 在 Hadoop 中非常容易地执行数据操纵操作。从这个[大数据课程](https://www.edureka.co/big-data-hadoop-training-certification)，你会学到更多关于猪、蜂巢、水槽等的知识。

Apache Pig 有两个主要组件——Pig 拉丁语语言和 **Pig 运行时环境**，Pig 拉丁语程序在其中执行。

Apache Pig 遵循 **ETL** (提取转换负载)过程。它可以处理不一致的模式(对于非结构化数据)。

【Apache Pig】自动优化执行前的任务，即自动优化。 阿帕奇猪处理各种数据。

Pig 允许程序员编写 Pig 中没有的自定义功能。用户自定义函数(UDF)可以用不同的语言编写，如 Java、Python、Ruby 等。并把它们嵌入到猪文字中。

Pig Latin 提供了各种内置运算符，如连接、排序、过滤等。读取、写入和处理大型数据集。

*♣提示:在阅读本阿帕奇猪面试问题之前，我建议你先阅读一下 **[阿帕奇猪教程](https://www.edureka.co/blog/pig-tutorial/)** 来修正你的猪概念。*

现在继续，让我们看看 Apache Pig 面试问题。

## **Hadoop 面试问答|爱德华卡**

## **1。强调 MapReduce 和 Apache Pig 之间的主要区别。**

♣提示:在这个问题中，你应该解释导致雅虎开发 Apache Pig 的 MapReduce 的问题。

 <caption>#### **MapReduce vs 阿帕奇猪**</caption> 
| **MapReduce** | **阿帕奇猪** |
| 1。这是一个低级数据处理范例 | 1。它是一个高级数据流平台 |
| 2。复杂的 Java 实现 | 2。没有复杂的 Java 实现 |
| 3。不提供嵌套数据类型 | 3。提供嵌套数据类型，如元组、包和映射 |
| 4。执行数据操作是一项巨大的任务 | 4。提供许多内置运算符来支持数据操作 |

## **2。阿帕奇猪有哪些用例？**

Apache Pig 用于分析和执行涉及临时处理的任务。阿帕奇猪用于:

*   研究大型原始数据集，如搜索平台的数据处理。例如，雅虎使用 Apache Pig 来分析从雅虎搜索引擎和雅虎新闻源收集的数据。
*   处理庞大的数据集，如网络日志、在线数据流等。
*   在电商网站这样的客户行为预测模型中。

## **3。逻辑计划和物理计划的区别是什么？**

♣提示:通过解释何时创建逻辑和物理计划来解决这个问题。

当 Pig 拉丁脚本被编译器转换成 MapReduce 作业时，Pig 会经历一些步骤。逻辑和物理计划是在 pig 脚本执行期间创建的。

在执行基本的解析和语义检查后，解析器产生一个逻辑计划，在逻辑计划的创建过程中不进行任何数据处理。逻辑计划描述了在执行过程中 Pig 必须执行的逻辑操作符。对于 Pig 脚本中的每一行，对操作符进行语法检查，并创建一个逻辑计划。如果遇到错误，就会抛出异常，程序执行结束。

一个逻辑计划包含脚本中操作符的集合，但不包含操作符之间的边。

生成逻辑计划后，脚本执行转移到物理计划，其中有关于 Apache Pig 将使用的物理操作符的描述，以执行 Pig 脚本。物理计划类似于一系列 MapReduce 作业，但是物理计划没有任何关于如何在 MapReduce 中执行的参考。

## **4。Pig 编程如何转换成 MapReduce 工作？**

Pig 是一个高级平台，使许多 Hadoop 数据分析问题更容易执行。用拉丁文写的程序是一种数据流语言，它需要一个执行引擎来执行查询。所以，当一个程序是用 Pig 拉丁语写的，Pig 编译器会把程序转换成 MapReduce 作业。

## **5。Pig 执行环境的组成有哪些？**

Apache Pig 执行环境的组件有:

*   **Pig 脚本** : Pig 脚本提交给 Apache Pig 执行环境，可以使用内置操作符用 Pig 拉丁语编写，可以嵌入 UDF。
*   **解析器**:解析器进行类型检查，并检查脚本的语法。解析器输出 DAG(有向无环图)。DAG 表示 Pig 拉丁语句和逻辑运算符。
*   **优化器** **:** 优化器执行诸如拆分、合并、转换、重新排序操作符等优化活动。优化器为 Apache Pig 提供了自动优化功能。优化器的基本目标是减少管道中的数据量。
*   **编译器**:Apache Pig 编译器自动将优化后的代码转换成 MapReduce 作业。
*   **执行引擎**:最后，MapReduce 作业提交给执行引擎。然后，执行 MapReduce 作业，并产生所需的结果。

## **6。猪八戒剧本有哪些不同的执行方式？**

有三种方法可以执行 Pig 脚本:

*   **Grunt Shell** :这是猪的交互 Shell，用来执行所有的猪脚本。
*   **脚本文件**:将所有 Pig 命令写入一个脚本文件，并执行 Pig 脚本文件。这是由 Pig 服务器执行的。
*   **嵌入式脚本**:如果一些函数在内置操作符中不可用，我们可以通过编程创建用户定义的函数(UDF)，使用其他语言如 Java、Python、Ruby 等来实现该功能。并将其嵌入到猪拉丁脚本文件中。然后，执行脚本文件。

## **7。猪拉丁的数据类型有哪些？**

Pig Latin 可以处理两种原子数据类型，如 int、float、long、double 等。以及复杂的数据类型，如 tuple、bag 和 map。

原子或标量数据类型是所有语言中使用的基本数据类型，如 string、int、float、long、double、char[]、byte[]。这些也被称为原始数据类型。

猪拉丁支持的复杂数据类型有:

*   **元组**:元组是一组有序的字段，每个字段可能包含不同的数据类型。
*   **包**:包是一组元组的集合，这些元组是表的行的子集或整行。
*   **Map**:Map 是用来表示数据元素的键值对。该键必须是 chararray []，并且应该像列名一样是唯一的，以便可以对其进行索引，并且可以根据这些键来访问与其相关联的值。该值可以是任何数据类型。

*♣提示:猪拉丁的复杂数据类型理解起来非常重要，可以通过 **[Apache 猪教程](https://www.edureka.co/blog/pig-tutorial/)** 博客深入了解。*

## **8。猪拉丁语中的包是什么？**

bag 是 Pig 中存在的数据模型之一。它是一个无序的元组集合，可能有重复的元组。分组时，包用于存储元组的集合。包的大小是本地磁盘的大小，这意味着包的大小是有限的。当包满时，Pig 会将这个包放入本地磁盘，只在内存中保留包的一部分。没有必要将整个包放入存储器中。我们用“{}”代表包。

♣提示:你也可以用拉丁语解释包的两种类型，即外袋和内袋，这可能会给你的雇主留下深刻印象。

## **9。你对猪身上的内袋和外袋的理解是什么？**

外层包或关系只不过是一包元组。这里的关系类似于关系数据库中的关系。比如:

*{(加州林肯公园)，(洛杉矶金属乐队)，(洛杉矶超级死神)}*

内包包含元组内的包。比如:

(洛杉矶， *{(金属乐队，洛杉矶)，(超级死神，洛杉矶)}* )

(加州， *{(加州林肯公园)}* )

## **10。Apache Pig 如何处理模式和无模式数据？**

♣提示:Apache Pig 既处理模式数据，也处理无模式数据。因此，这是一个值得关注的重要问题。

Apache Pig 既处理模式数据，也处理无模式数据。

*   如果模式只包括字段名，字段的数据类型被认为是字节数组。
*   如果您为字段指定了名称，您可以通过字段名和位置符号来访问该字段，而如果缺少字段名，我们只能通过位置符号来访问它，即$后跟索引号。
*   如果您执行任何关系组合的操作(如连接、共组等)。)并且如果任何关系缺少模式，则产生的关系将具有空模式。
*   如果模式为空，Pig 会将其视为字节数组，字段的实际数据类型将动态确定。

## **11。用户如何在 Apache Pig 中与 shell 交互？**

使用 Grunt，即 Apache Pig 的交互 shell，用户可以与 HDFS 或本地文件系统进行交互。

用户应使用*pig–x local*命令启动 Grunt。这个命令将提示 Grunt shell。要退出 grunt shell，请按 CTRL+D 或直接键入 exit。

## **12。什么是 UDF？**

如果一些函数在内置操作符中不可用，我们可以通过编程创建用户定义的函数(UDF ),使用其他语言如 Java、Python、Ruby 等来实现该功能。并将其嵌入到猪拉丁脚本文件中。

*♣提示:要了解如何创建和使用 UDF，请浏览这篇博客—**[在 Apache Pig 中创建 UDF](https://www.edureka.co/blog/creating-udf-in-apache-pig/)。***

*♣提示:关于 UDF 需要重点关注的要点:*

*   LoadFunc 抽象类有三个加载数据的主要方法，对于大多数用例来说，扩展它就足够了。
*   LoadPush 具有将 Pig 运行时的操作推入加载器实现的方法。
*   前端和后端的 Pig 都会调用 setUdfContextSignature()方法，将唯一的签名传递给加载程序。
*   加载/存储 UDF 控制数据如何进出 Pig。
*   get next()的含义是 Pig 运行时调用，获取数据中的下一个元组。
*   加载程序应该使用 setLocation()方法将加载信息传递给底层的 InputFormat。
*   prepareToRead 方法使得与 LoadFunc 提供的 InputFormat 相关联的 RecordReader 被传递给 LoadFunc。然后，getNext()中的实现可以使用 RecordReader 将表示数据记录的元组返回给 pig。
*   pushProjection()方法告诉 load func Pig 脚本中需要哪些字段。Pig 将使用列索引 requiredField.index 与 LoadFunc 就 Pig 脚本所需的字段进行通信。
*   LoadCaster 有将字节数组转换成特定类型的方法。
*   如果需要支持从 DataByteArray 字段到其他类型的强制转换(隐式或显式),加载器实现应该实现 LoadCaster()。LoadCaster 有将字节数组转换成特定类型的方法。

## **13。列出 Pig 中的诊断操作员。**

Pig 支持许多诊断运算符，您可以使用这些运算符来调试 Pig 脚本。

*   **转储:**显示与屏幕相关的内容。
*   **描述:**返回一个关系的模式。
*   **解释:**显示逻辑、物理和 MapReduce 执行计划。
*   **说明:**给出了一系列语句的分步执行。

*♣提示:浏览这个关于 [**诊断运算符**](https://www.edureka.co/blog/operators-in-apache-pig-diagnostic-operators/) 的博客，了解它们并看看它们的实现。*

## **14。“插图”是否运行 MapReduce 作业？**

不，illustrate 不会拉任何 MapReduce，它会拉内部数据。在控制台上，说明不会做任何工作。它只是显示每个阶段的输出，而不是最终输出。

图解运算符用于查看数据如何通过一系列 Pig 拉丁文语句进行转换。在调试脚本时，ILLUSTRATE command 是您最好的朋友。这个命令本身可能就是选择 Pig 而不是其他东西的一个很好的理由。

语法:*图解 relation _ name*

## **15。《阿帕奇猪》中的插图是做什么的？**

在大型数据集上执行 Pig 脚本通常需要很长时间。为了解决这个问题，开发人员对样本数据运行 Pig 脚本，但是所选的样本数据可能无法正确执行您的 Pig 脚本。例如，如果脚本有一个连接操作符，那么样本数据中至少应该有几条记录具有相同的键，否则连接操作将不会返回任何结果。

为了解决这类问题，使用了图解。Illustrate 获取数据样本，每当遇到删除数据的操作符(如 join 或 filter)时，它会通过修改记录使其满足条件，从而确保只有一些记录通过，而一些记录没有通过。Illustrate 只显示每个阶段的输出，但不运行任何 MapReduce 任务。

## **16。列出 Pig 中的关系运算符。**

所有的猪拉丁语句都是对关系进行操作的(而操作符叫做关系操作符)。猪拉丁文中不同的关系运算符有:

*   **协同分组**:将两个或多个表格合并，然后对合并后的表格结果进行分组运算。
*   **交叉**:交叉运算符用于计算两个或多个关系的叉积(笛卡尔积)。
*   **DISTINCT** :删除关系中重复的元组。
*   **过滤器**:根据条件从关系中选择一组元组。
*   **FOREACH** :迭代一个关系的元组，生成一个数据转换。
*   **分组**:将一个或多个关系中的数据分组。
*   **连接**:连接两个或多个关系(内部或外部连接)。
*   **限制**:限制输出元组的数量。
*   **加载**:从文件系统加载数据。
*   **排序**:根据一个或多个字段对关系进行排序。
*   **分割**:将一个关系分割成两个或多个关系。
*   **存储**:将数据存储在文件系统中。
*   **联合**:合并两个关系的内容。要对两个关系执行联合运算，它们的列和域必须相同。

*♣提示:浏览这个关于 [**关系运算符**](https://www.edureka.co/blog/operators-in-apache-pig/) 的博客，了解它们并看看它们的实现。*

## **17。关键字‘DEFINE’像函数名吗？**

是的，关键字‘DEFINE’就像一个函数名。

DEFINE 语句用于为 UDF 功能或流命令指定一个名称(别名)。

*   函数有一个很长的包名，你不希望它包含在脚本中，尤其是当你在脚本中多次调用函数的时候。该函数的构造函数接受字符串参数。如果您需要为不同的函数调用使用不同的构造函数参数，您将需要创建多个定义——每个参数集一个。
*   流命令规范很复杂。流式命令规范需要额外的参数(输入、输出等)。因此，分配一个别名可以使访问变得更容易。

**18。猪的辅群有什么作用？**

COGROUP 获取不同关系的成员，通过相似的字段绑定它们，并创建一个包含两个关系的单个实例的包，其中这些关系具有公共字段。协同分组操作通过仅将一个特定的数据集分组来加入数据集。

它根据公共字段对元素进行分组，然后返回一组包含两个独立包的记录。第一包由具有公共数据集的第一数据集记录组成，第二包由具有公共数据集的第二数据集记录组成。

## **19。我们能说 co-group 是一个多于 1 个数据集的组吗？**

Co-group 是一组数据集。不止一个数据集，co-group 将所有数据集分组，并根据公共字段连接它们。因此，我们可以说，协同组是一个以上数据集的组，也是该数据集的联接。

## **20。Pig 中的群算子和余群算子的区别？**

Group 和 Cogroup 运算符是相同的。为了可读性，GROUP 用在涉及一个关系的语句中，COGROUP 用在涉及两个或更多关系的语句中。Group operator 收集具有相同关键字的所有记录。Cogroup 是 group 和 join 的组合，它是 group 的一般化，而不是收集一个输入依赖于一个键的记录，它收集基于一个键的 n 个输入的记录。一次，我们最多可以组合 127 个关系。

## **21。您在 HDFS 目录中有一个包含 100 条记录的个人数据文件。您只想查看 employee.txt 文件中的前 5 条记录。你会怎么做？**

为了从 100 条记录中只获得 5 条记录，我们使用了极限运算符。

首先将数据装入清管器:

*personal _ data = LOAD "/personal _ data . txt "使用 PigStorage('，')作为(parameter1，Parameter2，…)；*

然后将数据限制为 5 条记录:

*LIMIT _ data = LIMIT personal _ data 5；*

## **22。什么是映射文件？**

MapFile 是一个提供从键到值的基于文件的映射的类。

映射是包含两个文件的目录，一个是数据文件，包含映射中的所有键和值，另一个是较小的索引文件，包含一小部分键。分数由*映射文件决定。*。

索引文件被完全读入内存。因此，关键实现应该尽量保持小规模。通过按顺序添加条目来创建映射文件。

## **23。BloomMapFile 是做什么用的？**

BloomMapFile 是一个扩展 MapFile 的类。所以它的功能类似于 MapFile。BloomMapFile 使用动态布隆过滤器为密钥提供快速成员测试。它用于 Hbase 表格式。

## **24。Pig 中有哪些不同的执行模式？**

Apache Pig 中的执行模式有:

*   **MapReduce 模式**:这是默认模式，需要访问 Hadoop 集群和 HDFS 安装。因为这是默认模式，所以没有必要指定-x 标志(您可以执行 pig 或 pig -x mapreduce)。这种模式下的输入和输出出现在 HDFS 上。
*   **本地模式:**访问单台机器，使用本地主机和文件系统安装并运行所有文件。这里，本地模式使用'-x flag' (pig -x local)指定。这种模式下的输入和输出存在于本地文件系统中。

## **25。猪脚本区分大小写吗？**

♣提示:解释 Apache Pig 的两个方面，即区分大小写和不区分大小写。

Pig 脚本既区分大小写又不区分大小写。

用户定义的函数、字段名和关系区分大小写，即雇员与雇员不同，或者 M =加载“数据”与 M =加载“数据”不同。

而 Pig 脚本关键字不区分大小写，即 load 与 LOAD 相同。

很难说 Apache Pig 是区分大小写还是不区分大小写。例如，Pig 中用户定义的函数、关系和字段名是区分大小写的。另一方面，Apache Pig 中的关键字不区分大小写。

## **26。扁平化在猪身上有什么作用？**

有时一个元组或一个包中有数据，如果我们想从该数据中移除嵌套层次，那么可以使用 Pig 中的 Flatten 修饰符。展平取消嵌套的包和元组。对于元组，Flatten 操作符将替换元组的字段来代替元组，而取消包的嵌套有点复杂，因为它需要创建新的元组。

## **27。什么是猪统计？Java API 包中所有可用的 stats 类是什么？**

Pig Statistics 是一个为 Pig Latin 收集和存储脚本级统计数据的框架。Pig Latin 脚本的特征和生成的 MapReduce 作业在脚本执行时收集。在任务完成后，Pig 用户和使用 Pig 的工具(如 Oozie)可以获得这些统计数据。

统计类在包*org . Apache . pig . tools . pig stats*:中

*   PigStats
*   JobStats
*   OutputStats
*   InputStats.

## **28。猪的局限性是什么？**

阿帕奇猪的局限性是:

1.  由于 Pig 平台是为 ETL 类型的用例而设计的，所以它不是实时场景的更好选择。
2.  Apache Pig 并不是在庞大的数据集中精确定位单个记录的好选择。
3.  Apache Pig 建立在 MapReduce 之上，MapReduce 是面向批处理的。

## **结论:**

我希望这些 Apache Pig 面试问题对你有所帮助。我建议您浏览整个系列，深入了解 Hadoop 面试问题。在使用真实用例的同时，向行业专家学习 Hadoop。

请参考下面给出的链接，享受阅读:

*   [***50 强 Hadoop 面试问题***](https://www.edureka.co/blog/interview-questions/top-50-hadoop-interview-questions-2016/)
*   [***Hadoop 集群面试题***](https://www.edureka.co/blog/interview-questions/hadoop-interview-questions-hadoop-cluster/)
*   **[*Hadoop HDFS 面试题*](https://www.edureka.co/blog/interview-questions/hadoop-interview-questions-hdfs-2/)**
*   [***Hadoop MapReduce 面试问题***](https://www.edureka.co/blog/interview-questions/hadoop-interview-questions-mapreduce/)
*   [***蜂巢面试问题***](https://www.edureka.co/blog/interview-questions/hive-interview-questions/)
*   ***[HBase 面试题](https://www.edureka.co/blog/interview-questions/hbase-interview-questions/)***

有问题要问我们吗？在评论区提到它们，我们会给你回复。