# 分类算法简介

> 原文:[https://www.edureka.co/blog/classification-algorithms/](https://www.edureka.co/blog/classification-algorithms/)

***分类算法*** 的思路相当简单*。*你通过分析训练数据集来预测目标类。这是你在学习数据科学时最重要的一个概念。T19】

本博客讨论了以下概念:

*   [**什么是分类？**](#whatisclassification)
*   [**分类 vs 聚类算法**](#classificationalgorithmsvsclusteringalgorithms)
*   [**分类算法中的基本术语**](#basicterminologyinclassificationalgorithms)
*   [**分类算法的应用**](#applicationsofclassificationalgorithms)
*   [**分类算法类型**](#typesofclassificationalgorithms)
*   [**逻辑回归**](#logisticregression)T5】
*   [**决策树**](#decisiontree)T5】
*   [**朴素贝叶斯分类器**](#naivebayesclassifier)T5】
*   [**K 最近邻**](#knn)T5】
*   [](#svm)

## **什么是阶级化？**

我们使用训练数据集来获得更好的边界条件，该边界条件可用于确定每个目标类。一旦确定了边界条件，下一个任务就是预测目标类。整个过程被称为分类。

**目标类例子:**

*   分析客户数据，预测他是否会购买电脑配件**(目标类别:是或否)**
*   根据颜色、味道、大小、重量等特征对水果进行分类**(目标类别:苹果、橙子、樱桃、香蕉)**
*   根据头发长度进行性别分类**(目标类别:男性或女性)**

让我们来理解使用头发长度 进行性别分类的分类算法的概念 (我绝不是试图用性别来刻板化，这只是一个例子)。为了使用毛发长度作为特征参数来分类性别**(目标类别)**，我们可以使用任何分类算法来训练模型，以提出一些边界条件集合，这些边界条件可以用于使用毛发长度作为训练特征来区分男性和女性。在性别分类的情况下，边界条件可以是适当的头发长度值。假设**区分边界**毛发长度值为 15.0 cm，那么我们可以说，如果毛发长度**小于 15.0 cm** ，那么性别可以是男性或者女性。

## **分类算法 vs 聚类算法**

在聚类中，其思想不是像在分类中那样预测目标类，而是更多地试图通过考虑最满意的条件来对同类事物进行分组，**同一组中的所有项目都应该是相似的，并且没有两个不同组的项目应该是不相似的。**T3】

**组项示例:**

*   对相似语言类型的文档进行分组时**(相同语言的文档为一组。)**
*   对新闻文章进行分类时**(同一新闻类(体育类)文章为一组)**

让我们以头发长度为例来理解性别聚类的概念。为了确定性别，可以使用不同相似性度量来对男性和女性进行分类。这可以通过找到两个头发长度之间的相似性来完成，并且如果相似性较小**(头发长度的差异较小)**，则将它们保持在同一组中。同样的过程可以继续下去，直到所有的头发长度适当地分为两类。

## **分类算法中的基本术语**

*   **分类器:**将输入数据映射到特定类别的算法。
*   **分类模型:**分类模型试图从给定的用于训练的输入值中得出一些结论。它将预测新数据的分类标签/类别。
*   **特征:**特征是被观察现象的个体可测量的属性。
*   **二元分类:**分类有两种可能结果的任务。**例如:性别分类(男/女)**
*   **多类分类:**用两个以上的类进行分类。在多类分类中，每个样本被分配给一个且仅一个目标标签。动物可以是猫或狗，但不能同时是猫和狗。
*   **多标签分类:**将每个样本映射到一组目标标签(不止一个类别)的分类任务。一篇新闻可以同时涉及运动、人物和地点。

## **分类算法的应用**

*   垃圾邮件分类
*   银行客户贷款支付意愿预测。
*   癌症肿瘤细胞鉴定。
*   情绪分析
*   药品分类
*   面部关键点检测
*   汽车行驶中的行人检测。

## **分类算法类型**

分类算法可以大致分为以下几种:

*   ***线性量词***
    *   逻辑回归
    *   朴素贝叶斯分类器
    *   费希尔线性判别式
*   ***支持向量机***
    *   最小二乘支持向量机
*   ***二次量词**T5】*
*   ***内核估计**T5】*
    *   k 近邻
*   ***决策树**T5】*
    *   随机森林
*   ***神经网络***
*   ***学习矢量量化**T5】*

下面给出了一些流行的分类算法的例子。

## **逻辑回归**

尽管这个名字可能令人困惑，但你可以放心。逻辑回归是一种分类，而不是回归算法。它根据给定的一组独立变量估计离散值**(二进制值，如 0/1、是/否、真/假)**。简单来说，它基本上是通过将数据拟合到 ***logit 函数*来预测事件发生的概率。**因此，又称 [***logit 回归***](https://www.edureka.co/blog/understanding-logistic-regression-in-r/) 。获得的值将总是位于 0 和 1 之间，因为它预测了概率。

让我们试着通过另一个例子来理解这一点。

假设你的数学测验有一道算术题。它只能有两种结果，对吗？要么你解决它，要么你不解决它(这里我们不假设方法的要点)。现在想象一下，给你一个大范围的总数，试图理解你已经很好地理解了哪些章节。这项研究的结果大概是这样的——如果给你一个基于三角学的问题，你有 70%的可能解决它。另一方面，如果是一道算术题，你得到答案的概率只有 30%。这就是逻辑回归给你提供的。

如果我必须做数学计算，我会将结果的对数概率建模为预测变量的线性组合。

```
odds= p/ (1-p) = probability of event occurrence / probability of event occurrence ln(odds) = ln(p/(1-p)) logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk)
```

在上面给出的等式中， *p* 是感兴趣特征出现的概率。它选择最大化观察样本值的可能性的参数，而不是最小化误差平方和的参数(就像普通回归一样)。

现在，很多人可能会问，为什么要用日志呢？为了简单起见，我们姑且说这是复制阶跃函数的最好的数学方法之一。我可以对此进行更深入的探讨，但这将违背我写这篇博客的目的。

### **R 代码:**

```
x <- cbind(x_train,y_train)
# Train the model using the training sets and check score logistic <- glm(y_train ~ ., data = x,family='binomial')
summary(logistic) #Predict Output
predicted= predict(logistic,x_test)
```

为了改进模型，可以尝试许多不同的步骤:

*   包括交互术语
*   移除特征
*   规则化技术
*   使用非线性模型

## **决策树**

现在， [***决策树***](https://www.edureka.co/blog/decision-trees/) 是目前为止，我最喜欢的算法之一。它具有多种多样的特征，有助于实现分类和连续因变量，是一种监督学习算法，主要用于分类问题。该算法所做的是，根据最重要的属性将种群分成两个或更多同类集合，使各组尽可能不同。

![Decision Tree - Classification Algorithms - Edureka](../Images/738ff3b9fa9d846d98f98fdbe2519900.png)T2】

在上图中，你可以看到人口根据多种属性被分为四个不同的组，以识别“他们是否会玩”。

### **R 代码:**

```
library(rpart)
x <- cbind(x_train,y_train)
# grow tree 
fit <- rpart(y_train ~ ., data = x,method="class") summary(fit) #Predict Output 
predicted= predict(fit,x_test)
```

## **朴素贝叶斯分类器**

这是一种基于预测器之间独立性假设或所谓的*贝叶斯定理*的分类技术。简单来说， [***朴素贝叶斯分类器***](https://www.edureka.co/blog/naive-bayes-classifier/) 假设一个类中特定特征的存在与任何其他特征的存在无关。

例如，如果一个水果是红色的，圆形的，直径大约 3 英寸，它就可以被认为是苹果。即使这些特征相互依赖或依赖于其他特征的存在，朴素贝叶斯分类器也会考虑所有这些属性，以独立地影响该水果是苹果的概率。

建立贝叶斯模型很简单，在巨大数据集的情况下特别有用。除了简单之外，朴素贝叶斯也被认为优于复杂的分类方法。

贝叶斯定理提供了从 **P(c)** 、 **P(x)** 和 **P(x|c)** 计算后验概率 **P(c|x)** 的方法。后验概率的表达式如下。

![Bayes Rule - Classification Algorithms - Edureka](../Images/b1ef945f9a740eee903e4ebdf519cd48.png)这里，

*   ***P* ( *c|x* )是给定*预测器* ( *属性*)的*类* ( *目标*)的后验概率。**T15】
*   ***P* ( *c* )是*类*的先验概率。**T9】
*   ***P* ( *x|c* )为似然，即*预测器*给定*类*的概率。**T11】
*   ***P* ( *x* )是*预测器*的先验概率。**T9】

**例子:**让我们通过一个例子来更好地理解这一点。因此，这里我有一个天气的训练数据集，即晴天、阴天和雨天，以及相应的二元变量“Play”。现在，我们需要根据天气情况对球员是否上场进行分类。让我们按照以下步骤来执行它。

**第一步:**将数据集转换成频率表

**第二步:**通过查找类似**阴天概率= 0.29****出场概率为 0.64** 的概率，创建一个可能性表。

**![Naive Bayes - Machine Learning Algorithms - Edureka](../Images/0e73618894bdbe76d680f6783be01af2.png)第三步:**现在，使用朴素贝叶斯方程计算每一类的后验概率。具有最高后验概率的类是预测的结果。

**问题:**玩家会在天气晴朗的时候玩，这种说法正确吗？

我们可以使用上面讨论的方法来解决它，所以 **P(是|阳光)= P(阳光|是)* P(是)/ P(阳光)**

这里我们有 **P (Sunny |Yes) = 3/9 = 0.33** ， **P(Sunny) = 5/14 = 0.36** ， **P( Yes)= 9/14 = 0.64**

现在， **P(是|晴)= 0.33 * 0.64 / 0.36 = 0.60** ，概率较大。

朴素贝叶斯使用类似的方法根据各种属性预测不同类别的概率。该算法主要用于文本分类和多类问题。

### **R 代码:**

```
library(e1071)
x <- cbind(x_train,y_train)
# Fitting model
fit <-naiveBayes(y_train ~ ., data = x)
summary(fit) #Predict Output 
predicted= predict(fit,x_test)
```

## KNN (k-最近邻)

***K 最近邻*** 是一个简单的算法，用于分类和回归问题。它基本上存储所有可用的案例，以通过其 k 个邻居的多数投票对新案例进行分类。分配给该类的事例在通过距离函数( 欧几里德、曼哈顿、闵可夫斯基和汉明)测量的 K 个最近邻中是最常见的。

前三种距离函数用于连续变量，而汉明距离函数用于分类变量。如果 **K = 1** ，那么该案例被简单地分配给其最近邻的类别。有时，在执行 kNN 建模时，选择 K 是一个挑战。

![KNN - Classification Algorithms - Edureka](../Images/9dadc9ee41b09d6d6e7774cd5a2a0aa6.png)T2】

举一个我们现实生活中的例子，你会很容易理解 KNN。如果你暗恋班上的一个女孩/男孩，而你对他/她一无所知，你可能想和他们的朋友和社交圈谈谈，以获得他们的信息！

### **R 代码:**

```
library(knn)
x <- cbind(x_train,y_train)
# Fitting model
fit <-knn(y_train ~ ., data = x,k=5)
summary(fit) #Predict Output 
predicted= predict(fit,x_test)
```

### **选择 KNN 之前要考虑的事情:**

*   KNN 的计算开销很大
*   变量应标准化，否则更高范围的变量会使其产生偏差
*   在像离群点一样进行 kNN 之前，更多地在预处理阶段工作，去除噪声

## **【支持向量机】**

在该算法中，我们将每个数据项绘制为 n 维空间中的一个点(其中 n 是您拥有的特征的数量)，每个特征的值是特定坐标的值。

例如，如果我们只有个人的身高和头发长度这两个特征，我们首先在二维空间中绘制这两个变量，其中每个点有两个坐标(这些坐标称为**支持向量** )

![SVM - Classification Algorithms - Edureka](../Images/1db85e1046dbfd6f86ceea90f151ad9a.png)现在，我们将找到一些*线*来分割两组不同分类的数据。这将是一条线，使得两组中的每一组离最近点的距离最远。

![SVM 2 - Classification Algorithms - Edureka](../Images/411c3736c2c6e0fd6e682bdef894a6a5.png)

在上面的例子中，将数据分成两个不同分类组的线是蓝色的*线*，因为两个最近的点离这条线最远。这一行是我们的*分类器*。然后，根据测试数据在线两边的位置，这就是我们可以将新数据分类的类别。

### **R 代码:**

```
library(e1071)
x <- cbind(x_train,y_train)
# Fitting model
fit <-svm(y_train ~ ., data = x)
summary(fit) #Predict Output 
predicted= predict(fit,x_test)
```

*那么，就这样，我们来结束这个分类算法的博客。现在在你的系统上尝试简单的 R 代码，你将不再称自己为这个概念的新手。T3】*