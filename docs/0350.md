# PySpark 数据帧教程——使用数据帧进行 PySpark 编程

> 原文:[https://www.edureka.co/blog/pyspark-dataframe-tutorial/](https://www.edureka.co/blog/pyspark-dataframe-tutorial/)

Dataframes 是当今业界的流行语。人们倾向于将它与用于数据分析的流行语言一起使用，如 Python、Scala 和 R. 加上处理复杂分析和大数据管理任务的明显需求，Python for Spark 或 ***[PySpark 认证](https://www.edureka.co/pyspark-certification-training)*** 已成为当今业界最受欢迎的技能之一。那么，为什么每个人都在如此频繁地使用它呢？让我们通过我们的 **PySpark Dataframe 教程**博客来理解这一点。在这篇博客中，我将涉及以下主题:

*   [**什么是数据帧？**](#what)
*   [**我们为什么需要数据帧？**](#why)
*   [**数据帧的特征**](#features)
*   [**PySpark Dataframe 来源**](#sources)
*   [**数据帧创建**](#creation)
*   [**【Pyspark】FIFA 世界杯数据帧&超级英雄数据集**](#fifa)

## **PySpark 数据帧教程:什么是数据帧？T3】**

Dataframes 通常指本质上是表格的数据结构。它表示行，每一行都由许多观察值组成。行可以有多种数据格式(**异构**)，而列可以有相同数据类型的数据(**同构**)。除数据外，数据帧通常还包含一些元数据；例如，列名和行名。

![Dataframe-Pyspark-Dataframe-Tutorial](../Images/69658703b15ed6c7712e26f71f0801b3.png)

我们可以说数据帧什么也不是，只是二维数据结构，类似于 SQL 表或电子表格。现在让我们继续学习 PySpark Dataframe 教程，并理解我们到底为什么需要 Pyspark Dataframe？

## **我们为什么需要数据帧？**T3】

**1。处理结构化和半结构化数据**T3

![Processing- Pyspark-Dataframe-Tutorial](../Images/a2b30adea79c6456df22b64362533b63.png)

数据帧被设计成处理一个**大的** **结构化以及半结构化数据的集合**。Spark 数据帧中的观察结果组织在命名列下，这有助于 Apache Spark 理解数据帧的模式。这有助于优化这些查询的执行计划。它还可以处理**Pb**的数据。

**2。S** **划片和划片**

![Slicing-Pyspark-Dataframe-Tutorial](../Images/715b382af7f4b5776e51ace8c65c189b.png)

数据帧 A PIs 通常支持对数据进行 **切片** 的精细方法。包括 操作 ns，如按名称或编号“选择”行、列、单元格，过滤出行等。统计数据通常非常混乱，包含大量缺失值、错误值和范围违规。因此，数据帧的一个至关重要的特性是对丢失数据的显式管理。

**3。数据来源**

![Data Sources - Pyspark Dataframe Tutorial](../Images/9e2e02c183573f8c1e98d4880d800e3b.png)

DataFrame 支持多种数据格式和数据源，我们将在稍后的 Pyspark Dataframe 教程博客中对此进行探讨。他们可以从各种来源获取数据。

**4。支持多种语言**T3】

![](../Images/0cae0e4fb9e22920cc72eabcb5fa0680.png)

它拥有对不同语言的 API 支持，如 Python、R、Scala、Java 、，这使得它更容易被不同编程背景的人使用。

#### 订阅我们的 youtube 频道以获取新的更新..！